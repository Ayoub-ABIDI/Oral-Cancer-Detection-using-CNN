{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b31c5a-ef78-47fa-a3d9-f32153046c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 131 files belonging to 2 classes.\n",
      "Using 105 files for training.\n",
      "Found 131 files belonging to 2 classes.\n",
      "Using 26 files for validation.\n",
      "Class names: []\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayoub\\OneDrive\\Desktop\\Oral Cancer Detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 432ms/step - accuracy: 0.6629 - loss: 3.1339 - val_accuracy: 0.5000 - val_loss: 2.9558\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 342ms/step - accuracy: 0.5369 - loss: 2.5116 - val_accuracy: 0.5000 - val_loss: 1.4215\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 346ms/step - accuracy: 0.6999 - loss: 0.6894 - val_accuracy: 0.5000 - val_loss: 0.9868\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 409ms/step - accuracy: 0.6786 - loss: 0.5473 - val_accuracy: 0.6154 - val_loss: 0.6470\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 348ms/step - accuracy: 0.8038 - loss: 0.4041 - val_accuracy: 0.4615 - val_loss: 0.7179\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 361ms/step - accuracy: 0.4453 - loss: 0.7802 - val_accuracy: 0.6154 - val_loss: 0.6723\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 337ms/step - accuracy: 0.7776 - loss: 0.4623 - val_accuracy: 0.6538 - val_loss: 0.6792\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 358ms/step - accuracy: 0.9267 - loss: 0.3851 - val_accuracy: 0.5000 - val_loss: 0.8340\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 342ms/step - accuracy: 0.8257 - loss: 0.3507 - val_accuracy: 0.6154 - val_loss: 0.8094\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435ms/step - accuracy: 0.8853 - loss: 0.3133 - val_accuracy: 0.5000 - val_loss: 1.2593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24d0ee67020>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "# Set the path to your dataset directory\n",
    "data_dir = \"DataSet\"  # <-- Replace this with your dataset path\n",
    "\n",
    "# Image dimensions\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# Define augmentation function\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image, label\n",
    "\n",
    "# Load the training dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Load the validation dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Apply preprocessing and augmentation to the training dataset\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(100)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the validation dataset\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Optional: View class names\n",
    "class_names = train_ds.class_names if hasattr(train_ds, \"class_names\") else []\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Sample model (you can replace this with your custom model)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd207f3b-8bd6-4b59-b991-af7da24f038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 131 files belonging to 2 classes.\n",
      "Using 105 files for training.\n",
      "Found 131 files belonging to 2 classes.\n",
      "Using 26 files for validation.\n",
      "Epoch 1/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 387ms/step - accuracy: 0.4847 - loss: 2.2584 - val_accuracy: 0.5000 - val_loss: 0.8992\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 342ms/step - accuracy: 0.4581 - loss: 0.8669 - val_accuracy: 0.5000 - val_loss: 0.7416\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 337ms/step - accuracy: 0.7115 - loss: 0.5507 - val_accuracy: 0.5000 - val_loss: 0.7123\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 321ms/step - accuracy: 0.7268 - loss: 0.5564 - val_accuracy: 0.5000 - val_loss: 0.6985\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - accuracy: 0.6741 - loss: 0.5275 - val_accuracy: 0.5000 - val_loss: 1.0181\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - accuracy: 0.7129 - loss: 0.4764 - val_accuracy: 0.5385 - val_loss: 0.8164\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 0.7523 - loss: 0.4545 - val_accuracy: 0.5769 - val_loss: 0.6945\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 284ms/step - accuracy: 0.8776 - loss: 0.3129 - val_accuracy: 0.5385 - val_loss: 0.9633\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 352ms/step - accuracy: 0.7776 - loss: 0.4309 - val_accuracy: 0.6154 - val_loss: 0.7066\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step - accuracy: 0.9566 - loss: 0.2482 - val_accuracy: 0.5000 - val_loss: 1.9367\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 280ms/step - accuracy: 0.8267 - loss: 0.3154 - val_accuracy: 0.6538 - val_loss: 0.7333\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376ms/step - accuracy: 0.9423 - loss: 0.2450 - val_accuracy: 0.6154 - val_loss: 0.9962\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 302ms/step - accuracy: 0.9414 - loss: 0.1848 - val_accuracy: 0.6154 - val_loss: 1.1152\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 0.9762 - loss: 0.1028 - val_accuracy: 0.5769 - val_loss: 0.8497\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 302ms/step - accuracy: 0.9910 - loss: 0.1258 - val_accuracy: 0.6538 - val_loss: 1.3315\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 299ms/step - accuracy: 0.9605 - loss: 0.1356 - val_accuracy: 0.6154 - val_loss: 1.1527\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 0.9634 - loss: 0.1158 - val_accuracy: 0.6923 - val_loss: 0.8628\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy: 0.9865 - loss: 0.1039 - val_accuracy: 0.6923 - val_loss: 0.8815\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401ms/step - accuracy: 0.9048 - loss: 0.3922 - val_accuracy: 0.5385 - val_loss: 1.6875\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 331ms/step - accuracy: 0.9667 - loss: 0.0889 - val_accuracy: 0.6923 - val_loss: 0.8046\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - accuracy: 0.9463 - loss: 0.1457 - val_accuracy: 0.5769 - val_loss: 1.3552\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 318ms/step - accuracy: 0.9660 - loss: 0.0960 - val_accuracy: 0.6154 - val_loss: 1.1133\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 377ms/step - accuracy: 0.9820 - loss: 0.0968 - val_accuracy: 0.7308 - val_loss: 0.6974\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 317ms/step - accuracy: 1.0000 - loss: 0.1412 - val_accuracy: 0.5000 - val_loss: 1.2370\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 0.9570 - loss: 0.1071 - val_accuracy: 0.7308 - val_loss: 0.8668\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step - accuracy: 0.9935 - loss: 0.0706 - val_accuracy: 0.5385 - val_loss: 1.3253\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378ms/step - accuracy: 0.9727 - loss: 0.0605 - val_accuracy: 0.5385 - val_loss: 1.4448\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370ms/step - accuracy: 0.9962 - loss: 0.0281 - val_accuracy: 0.7308 - val_loss: 0.9627\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 312ms/step - accuracy: 0.9962 - loss: 0.0283 - val_accuracy: 0.6538 - val_loss: 1.2250\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 380ms/step - accuracy: 1.0000 - loss: 0.0259 - val_accuracy: 0.6923 - val_loss: 1.1558\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 383ms/step - accuracy: 0.9935 - loss: 0.0169 - val_accuracy: 0.6923 - val_loss: 1.2581\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 0.9941 - loss: 0.0190 - val_accuracy: 0.6923 - val_loss: 1.2496\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.6538 - val_loss: 1.4266\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377ms/step - accuracy: 1.0000 - loss: 0.0136 - val_accuracy: 0.6538 - val_loss: 1.5510\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 0.0059 - val_accuracy: 0.6538 - val_loss: 1.3849\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.6538 - val_loss: 1.3617\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 320ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.6923 - val_loss: 1.4945\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 0.6538 - val_loss: 1.6184\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376ms/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 0.6154 - val_loss: 1.7250\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 310ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 0.6154 - val_loss: 1.8588\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368ms/step - accuracy: 1.0000 - loss: 0.0205 - val_accuracy: 0.6538 - val_loss: 1.2744\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 315ms/step - accuracy: 1.0000 - loss: 0.0213 - val_accuracy: 0.6538 - val_loss: 1.9543\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 384ms/step - accuracy: 0.9896 - loss: 0.0183 - val_accuracy: 0.6538 - val_loss: 1.3196\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 311ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.6538 - val_loss: 1.0945\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step - accuracy: 1.0000 - loss: 0.0428 - val_accuracy: 0.6154 - val_loss: 1.6929\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 0.0193 - val_accuracy: 0.6923 - val_loss: 1.2931\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.6923 - val_loss: 1.3926\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 318ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.6923 - val_loss: 1.7187\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 0.6154 - val_loss: 2.2363\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 319ms/step - accuracy: 0.9941 - loss: 0.0174 - val_accuracy: 0.6154 - val_loss: 2.0585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.6154 - loss: 2.0585\n",
      "Validation accuracy: 0.6154\n",
      "Model saved as my_image_classifier.keras\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set the path to your dataset directory\n",
    "data_dir = \"DataSet\"  # <-- Replace with your dataset path\n",
    "\n",
    "# Image dimensions and batch size\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [img_height, img_width])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# Augmentation function\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image, label\n",
    "\n",
    "# Load datasets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\"\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(100)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model with accuracy metric\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=50)\n",
    "\n",
    "# Evaluate model on validation set\n",
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model in .keras format\n",
    "model.save(\"my_image_classifier.keras\")\n",
    "print(\"Model saved as my_image_classifier.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59079a-5474-4371-9a4c-6d1272b418b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
